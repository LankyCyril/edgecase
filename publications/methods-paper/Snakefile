from pandas import read_fwf, read_csv, concat
from io import StringIO
from re import sub, escape
from statsmodels.stats.multitest import multipletests
from scipy.stats import combine_pvalues, entropy


HG38EXT_ECX = "data/references/hg38/hg38ext.fa.ecx"
DATA_DIR = "data/datasets/GIAB-2020"
MAX_READ_LENGTH = 100000

DATASETS = read_fwf(StringIO(str.strip("""
group           subject  dataset
NA12878         HG001    11kb
AshkenazimTrio  HG002    10kb
AshkenazimTrio  HG002    15kb
AshkenazimTrio  HG002    15kb_20kb
AshkenazimTrio  HG003    15kb
AshkenazimTrio  HG003    15kb_20kb
AshkenazimTrio  HG004    15kb
AshkenazimTrio  HG004    15kb_20kb
AshkenazimTrio  HG004    15kb_21kb
ChineseTrio     HG005    11kb
ChineseTrio     HG006    15kb_20kb
ChineseTrio     HG006    hifi_google
ChineseTrio     HG007    15kb_20kb
ChineseTrio     HG007    hifi_google
""")))

DATASETS["subject_pacbio_path"] = DATASETS.apply(
    lambda row: "{}/PacBio/{}/{}".format(DATA_DIR, *row[:2]), axis=1,
)
DATASETS["dataset_pacbio_path"] = DATASETS.apply(
    lambda row: "{}/PacBio/{}/{}/{}".format(DATA_DIR, *row[:3]), axis=1,
)

wildcard_constraints:
    group="[^/]+", subject="[^/]+", dataset="[^/]+", name="[^/]+",


rule tailpuller:
    input:
        bam=DATA_DIR+"/PacBio/{group}/{subject}/{dataset}/wgs.bam",
        bai=DATA_DIR+"/PacBio/{group}/{subject}/{dataset}/wgs.bam.bai",
    output:
        bam=DATA_DIR+"/PacBio/{group}/{subject}/{dataset}/tailpuller.bam",
    params:
        max_read_length=MAX_READ_LENGTH,
    shell: """
        ./edgecase tailpuller \
            -x {HG38EXT_ECX} -m {params.max_read_length} -F3844 {input.bam} \
        | samtools view -bh > {output.bam}
    """

rule tailchopper:
    input:
        bam=DATA_DIR+"/PacBio/{group}/{subject}/{dataset}/tailpuller.bam",
    output:
        bam=DATA_DIR+"/PacBio/{group}/{subject}/{dataset}/tailchopper.bam",
    shell: """
        ./edgecase tailchopper -x {HG38EXT_ECX} -t tract_anchor {input.bam} \
        | samtools view -bh > {output.bam}
    """

def bam_combiner(w):
    return [f"{pacbio_path}/{w.name}.bam" for pacbio_path in DATASETS.loc[
        (DATASETS["group"]==w.group) & (DATASETS["subject"]==w.subject),
        "dataset_pacbio_path",
    ]]
rule combined_bam:
    input: bams=bam_combiner,
    output: bam=DATA_DIR+"/PacBio/{group}/{subject}/{name}.bam",
    params: sam=DATA_DIR+"/PacBio/{group}/{subject}/{name}.sam",
    run:
        shell("samtools view -H {input.bams[0]} > {params.sam}")
        for bam in input.bams:
            shell("samtools view {bam} >> {params.sam}")
        shell("samtools view -bh {params.sam} > {output.bam}")
        shell("rm {params.sam}")
rule combined_bams:
    input:
        p={spp+"/tailpuller.bam" for spp in DATASETS["subject_pacbio_path"]},
        c={spp+"/tailchopper.bam" for spp in DATASETS["subject_pacbio_path"]},


rule repeatfinder:
    input: bam=DATA_DIR+"/PacBio/{group}/{subject}/tailchopper.bam",
    output: tsv=DATA_DIR+"/PacBio/{group}/{subject}/repeatfinder-{arm}.tsv",
    params: s="4G", min_k=4, max_k=16, max_p_adjusted=1.1,
    threads: 12,
    run:
        if wildcards.arm == "p_arm":
            flags = "-F 'is_q|3844'"
        elif wildcards.arm == "q_arm":
            flags = "-f is_q -F 3844"
        else:
            raise ValueError("wildcards.arm", wildcards.arm)
        shell("""
            ./edgecase repeatfinder -j {threads} -s {params.s} \
                -m {params.min_k} -M {params.max_k} -P {params.max_p_adjusted} \
                {flags} {input.bam} > {output.tsv}
        """)


rule repeatfinder_all:
    input:
        p_arm=[
            DATA_DIR+f"/PacBio/{group}/{subject}/repeatfinder-p_arm.tsv"
            for _, (group, subject)
            in DATASETS[["group", "subject"]].drop_duplicates().iterrows()
        ],
        q_arm=[
            DATA_DIR+f"/PacBio/{group}/{subject}/repeatfinder-q_arm.tsv"
            for _, (group, subject)
            in DATASETS[["group", "subject"]].drop_duplicates().iterrows()
        ],
    output:
        p_arm=DATA_DIR+"/PacBio/repeatfinder-p_arm.tsv",
        q_arm=DATA_DIR+"/PacBio/repeatfinder-q_arm.tsv",
    run:
        pivot_p = dict(index="#monomer", columns="subject", values="p")
        pivot_a = dict(index="#monomer", columns="subject", values="abundance")
        arm_rfs, combined_p_as_list = {}, []
        for arm in "p_arm", "q_arm":
            arm_rf_as_list = []
            for tsv in getattr(input, arm):
                sample_rf = read_csv(tsv, sep="\t", usecols=(0,4,5))
                rpath = sub(r'^'+escape(DATA_DIR)+r'/PacBio/', "", tsv)
                sample_rf["group"], sample_rf["subject"], *_ = rpath.split("/")
                arm_rf_as_list.append(sample_rf)
            arm_rf = concat(arm_rf_as_list)
            arm_rf_by_p = arm_rf[pivot_p.values()].pivot(**pivot_p).fillna(1)
            mg = lambda row: combine_pvalues(row, method="mudholkar_george")[1]
            arm_combined_p = arm_rf_by_p.apply(mg, axis=1).fillna(1).to_frame(
                name="combined_p",
            )
            arm_combined_p["arm"] = arm
            arm_rfs[arm] = arm_rf
            combined_p_as_list.append(arm_combined_p)
        combined_p = concat(combined_p_as_list)
        mt_kws = dict(pvals=combined_p["combined_p"], method="bonferroni")
        combined_p["combined_p_adjusted"] = multipletests(**mt_kws)[1]
        for arm, arm_rf in arm_rfs.items():
            arm_rf_wide = concat([
                arm_rf[pivot_a.values()].pivot(**pivot_a).fillna(0),
                combined_p.loc[combined_p["arm"]==arm, ["combined_p_adjusted"]],
            ], axis=1)
            condition = (arm_rf_wide["combined_p_adjusted"]<.05)
            arm_rf_wide_filtered = arm_rf_wide[condition].copy()
            abund_mean = arm_rf_wide_filtered.iloc[:,:-1].mean(axis=1)
            arm_rf_wide_filtered["mean"] = abund_mean
            arm_rf_wide_final = (
                arm_rf_wide_filtered.sort_values(by="mean", ascending=False)
                .drop(columns="mean")
            )
            arm_rf_wide_final.to_csv(getattr(output, arm), sep="\t")
