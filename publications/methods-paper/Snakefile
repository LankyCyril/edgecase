from pandas import read_fwf, read_csv, concat
from io import StringIO
from re import sub, escape
from statsmodels.stats.multitest import multipletests
from scipy.stats import combine_pvalues, entropy


HG38EXT_ECX = "data/references/hg38/hg38ext.fa.ecx"
DATA_DIR = "data/datasets/GIAB-2020"
MAX_READ_LENGTH = 100000
MIN_PLOT_COVERAGE = 25

DATASETS = read_fwf(StringIO(str.strip("""
group           subject  dataset
NA12878         HG001    11kb
AshkenazimTrio  HG002    10kb
AshkenazimTrio  HG002    15kb
AshkenazimTrio  HG002    15kb_20kb
AshkenazimTrio  HG003    15kb
AshkenazimTrio  HG003    15kb_20kb
AshkenazimTrio  HG004    15kb
AshkenazimTrio  HG004    15kb_20kb
AshkenazimTrio  HG004    15kb_21kb
ChineseTrio     HG005    11kb
ChineseTrio     HG006    15kb_20kb
ChineseTrio     HG006    hifi_google
ChineseTrio     HG007    15kb_20kb
ChineseTrio     HG007    hifi_google
""")))

DATASETS["subject_pacbio_path"] = DATASETS.apply(
    lambda row: "{}/PacBio/{}/{}".format(DATA_DIR, *row[:2]), axis=1,
)
DATASETS["dataset_pacbio_path"] = DATASETS.apply(
    lambda row: "{}/PacBio/{}/{}/{}".format(DATA_DIR, *row[:3]), axis=1,
)

SMALLEST_P_VALUE = 5e-324
make_pvals_worse = lambda p: p if p != 0 else SMALLEST_P_VALUE

wildcard_constraints:
    group="[^/]+", subject="[^/]+", dataset="[^/]+", name="[^/]+",


def get_sam_flags(arm, target=None):
    if target:
        if arm == "p_arm":
            return "-f '{}' -F 'is_q|3844'".format(target)
        elif arm == "q_arm":
            return "-f 'is_q|{}' -F 3844".format(target)
        else:
            raise ValueError("arm", arm)
    else:
        if arm == "p_arm":
            return "-F 'is_q|3844'"
        elif arm == "q_arm":
            return "-f is_q -F 3844"
        else:
            raise ValueError("arm", arm)


rule tailpuller:
    input:
        bam=DATA_DIR+"/PacBio/{group}/{subject}/{dataset}/wgs.bam",
        bai=DATA_DIR+"/PacBio/{group}/{subject}/{dataset}/wgs.bam.bai",
    output:
        bam=DATA_DIR+"/PacBio/{group}/{subject}/{dataset}/tailpuller.bam",
    params:
        max_read_length=MAX_READ_LENGTH,
    shell: """
        ./edgecase tailpuller \
            -x {HG38EXT_ECX} -m {params.max_read_length} -F3844 {input.bam} \
        | samtools view -bh > {output.bam}
    """

rule tailchopper:
    input:
        bam=DATA_DIR+"/PacBio/{group}/{subject}/{dataset}/tailpuller.bam",
    output:
        bam=DATA_DIR+"/PacBio/{group}/{subject}/{dataset}/tailchopper.bam",
    shell: """
        ./edgecase tailchopper -x {HG38EXT_ECX} -t tract_anchor {input.bam} \
        | samtools view -bh > {output.bam}
    """

def bam_combiner(w):
    return [f"{pacbio_path}/{w.name}.bam" for pacbio_path in DATASETS.loc[
        (DATASETS["group"]==w.group) & (DATASETS["subject"]==w.subject),
        "dataset_pacbio_path",
    ]]
rule combined_bam:
    input: bams=bam_combiner,
    output: bam=DATA_DIR+"/PacBio/{group}/{subject}/{name}.bam",
    params: sam=DATA_DIR+"/PacBio/{group}/{subject}/{name}.sam",
    run:
        shell("samtools view -H {input.bams[0]} > {params.sam}")
        for bam in input.bams:
            shell("samtools view {bam} >> {params.sam}")
        shell("samtools view -bh {params.sam} > {output.bam}")
        shell("rm {params.sam}")
rule combined_bams:
    input:
        p={spp+"/tailpuller.bam" for spp in DATASETS["subject_pacbio_path"]},
        c={spp+"/tailchopper.bam" for spp in DATASETS["subject_pacbio_path"]},


rule repeatfinder:
    input: bam=DATA_DIR+"/PacBio/{group}/{subject}/tailchopper.bam",
    output: tsv=DATA_DIR+"/PacBio/{group}/{subject}/repeatfinder-{arm}.tsv",
    params: s="4G", min_k=4, max_k=16, max_p_adjusted=1.1,
    threads: 12,
    run:
        flags = get_sam_flags(wildcards.arm)
        shell("""
            ./edgecase repeatfinder -j {threads} -s {params.s} \
                -m {params.min_k} -M {params.max_k} -P {params.max_p_adjusted} \
                {flags} {input.bam} > {output.tsv}
        """)


rule repeatfinder_all:
    input:
        p_arm=[
            DATA_DIR+f"/PacBio/{group}/{subject}/repeatfinder-p_arm.tsv"
            for _, (group, subject)
            in DATASETS[["group", "subject"]].drop_duplicates().iterrows()
        ],
        q_arm=[
            DATA_DIR+f"/PacBio/{group}/{subject}/repeatfinder-q_arm.tsv"
            for _, (group, subject)
            in DATASETS[["group", "subject"]].drop_duplicates().iterrows()
        ],
    output:
        p_arm=DATA_DIR+"/PacBio/repeatfinder-p_arm-unadjusted.tsv",
        q_arm=DATA_DIR+"/PacBio/repeatfinder-q_arm-unadjusted.tsv",
    run:
        pivot_p = dict(index="#monomer", columns="subject", values="p")
        pivot_a = dict(index="#monomer", columns="subject", values="abundance")
        for arm in "p_arm", "q_arm":
            arm_rf_as_list = []
            for tsv in getattr(input, arm):
                sample_rf = read_csv(tsv, sep="\t", usecols=(0,4,5))
                rpath = sub(r'^'+escape(DATA_DIR)+r'/PacBio/', "", tsv)
                sample_rf["group"], sample_rf["subject"], *_ = rpath.split("/")
                arm_rf_as_list.append(sample_rf)
            arm_rf = concat(arm_rf_as_list)
            arm_rf["p"] = arm_rf["p"].apply(make_pvals_worse)
            arm_rf_by_p = arm_rf[pivot_p.values()].pivot(**pivot_p).fillna(1)
            arm_rf_by_a = arm_rf[pivot_a.values()].pivot(**pivot_a).fillna(0)
            assert (arm_rf_by_p.index == arm_rf_by_a.index).all()
            mg = lambda row: combine_pvalues(row, method="mudholkar_george")[1]
            arm_rf_by_a["p"] = arm_rf_by_p.apply(mg, axis=1).fillna(1)
            arm_rf_by_a.to_csv(getattr(output, arm), sep="\t")


rule adjust_all_repeatfinder:
    input:
        DATA_DIR+"/PacBio/repeatfinder-p_arm-unadjusted.tsv",
        DATA_DIR+"/PacBio/repeatfinder-q_arm-unadjusted.tsv",
    output:
        DATA_DIR+"/PacBio/repeatfinder-p_arm.tsv",
        DATA_DIR+"/PacBio/repeatfinder-q_arm.tsv",
    run:
        pvals = concat([read_csv(tsv, sep="\t")["p"] for tsv in input], axis=0)
        p_adjusted = multipletests(pvals, method="bonferroni")[1]
        bonferroni_lookup = {p: padj for p, padj in zip(pvals, p_adjusted)}
        for tsv in input:
            rf = read_csv(tsv, sep="\t")
            rf["p_adjusted"] = rf["p"].map(bonferroni_lookup)
            rf_filtered = rf[rf["p_adjusted"]<.05].drop(columns="p").copy()
            rf_filtered["mean"] = (
                rf_filtered.drop(columns="p_adjusted").mean(axis=1)
            )
            rf_sorted = rf_filtered.sort_values(by="mean", ascending=False)
            rf_sorted.drop(columns="mean").to_csv(
                sub(r'-unadjusted', "", tsv), sep="\t", index=False,
            )


rule plottable_repeats:
    input:
        p_arm=DATA_DIR+"/PacBio/repeatfinder-p_arm.tsv",
        q_arm=DATA_DIR+"/PacBio/repeatfinder-q_arm.tsv",
    output:
        p_arm=DATA_DIR+"/PacBio/repeatfinder-p_arm-plottable.tsv",
        q_arm=DATA_DIR+"/PacBio/repeatfinder-q_arm-plottable.tsv",
    params: n=4
    run:
        for arm in "p_arm", "q_arm":
            rf = read_csv(getattr(input, arm), sep="\t")
            top_rf = rf.rename(columns={"#monomer": "#motif"})[:params.n].copy()
            top_rf["abundance"] = (
                top_rf.drop(columns=["#motif", "p_adjusted"], errors="ignore")
                .mean(axis=1)
            )
            top_rf[["#motif", "abundance"]].to_csv(
                getattr(output, arm), sep="\t", index=False,
            )


rule kmerscanner_unfiltered:
    input:
        bam=DATA_DIR+"/PacBio/{group}/{subject}/tailpuller.bam",
        tsv=DATA_DIR+"/PacBio/repeatfinder-{arm}-plottable.tsv",
    output:
        dat=temp(DATA_DIR+"/PacBio/{group}/{subject}/kmerscanner-{arm}-unfiltered.dat.gz"),
    run:
        flags = get_sam_flags(wildcards.arm, target="tract_anchor")
        shell("""
            ./edgecase kmerscanner --motif-file {input.tsv} -w 10 \
                {flags} {input.bam} | gzip -2 > {output.dat}
        """)


rule kmerscanner_filtered:
    input: dat=DATA_DIR+"/PacBio/{group}/{subject}/kmerscanner-{arm}-unfiltered.dat.gz",
    output: dat=DATA_DIR+"/PacBio/{group}/{subject}/kmerscanner-{arm}.dat.gz",
    params: min_reads=MIN_PLOT_COVERAGE,
    run:
        raw_densities = read_csv(input.dat, sep="\t")
        chromosome_counter = raw_densities[["#name", "chrom"]].drop_duplicates()
        chromosome_counts = chromosome_counter["chrom"].value_counts()
        indexer = (chromosome_counts>=params.min_reads)
        chromosomes_to_keep = chromosome_counts[indexer].index
        filtered_densities = raw_densities[
            raw_densities["chrom"].isin(chromosomes_to_keep)
        ]
        filtered_densities.to_csv(
            output.dat, compression="gzip", sep="\t", index=False,
        )


rule densityplot:
    input: dat=DATA_DIR+"/PacBio/{group}/{subject}/kmerscanner-{arm}.dat.gz",
    output: pdf=DATA_DIR+"/PacBio/{group}/{subject}/densityplot-{arm}.pdf",
    run:
        flags = get_sam_flags(wildcards.arm, target="tract_anchor")
        shell("""
            ./edgecase densityplot \
                -x {HG38EXT_ECX} --palette paper --title ' ' {flags} \
                -z {input} > {output}
        """)
